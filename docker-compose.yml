version: '3.7'
services:

  minio:
    hostname: minio
    image: 'minio/minio:latest'
    container_name: minio
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - minio-data:/data
    environment:
      MINIO_ACCESS_KEY: minio
      MINIO_SECRET_KEY: minio123
    command: server /data --console-address ":9001"
    networks:
      - lakehouse

  postgres:
    build:
      context: postgres
    restart: always
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready", "-U", "hive", "-d", "metastore_db" ]
      interval: 30s
      timeout: 60s
      retries: 5
      start_period: 80s
    environment:
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive123
      - POSTGRES_DB=metastore_db
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - lakehouse

  hive-metastore:
    build:
      context: hive-metastore
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - HIVE_CUSTOM_CONF_DIR=/hive_custom_conf

    ports:
      - "9083:9083"
    volumes:
      - ./hive-metastore/warehouse:/opt/hive/data/warehouse
      - ./hive-metastore/conf:/hive_custom_conf
    networks:
      - lakehouse

  trino:
    image: trinodb/trino:latest
    ports:
      - "8090:8080"
    volumes:
      - ./trino/etc:/etc/trino
    networks:
      - lakehouse

  spark-master:
    build:
      context: spark
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_LOCAL_IP=spark-master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"
      - "9090:8080"
      - "9091:4040"
      - "10000:10000"
    networks:
      - lakehouse

  spark-worker-1:
    build:
      context: spark
    container_name: "spark-worker-1"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - lakehouse

  airflow-standalone:
    build:
      context: spark
    entrypoint: ["bash", "-c", "airflow standalone"]
    ports:
      - "8088:8080"
    volumes:
      - ./spark/dags:/airflow/dags
      - ./spark/jobs:/airflow/jobs
      - ./spark/connections.yml:/airflow/connections.yml
      - ./spark/airflow.cfg:/airflow/airflow.cfg
    networks:
      - lakehouse

  delta:
    image: deltaio/delta-docker:latest
    entrypoint: [ 'bash', 'pyspark',
                  '--packages', 'io.delta:delta-core_2.12:2.3.0,org.apache.hadoop:hadoop-aws:3.3.2',
                  '--conf', 'spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider',
                  '--conf', 'spark.hadoop.fs.s3a.access.key=minio',
                  '--conf', 'spark.hadoop.fs.s3a.secret.key=minio123',
                  '--conf', 'spark.hadoop.fs.s3a.endpoint=http://minio:9000',
                  '--conf', 'spark.hadoop.fs.s3a.path.style.access=true',
                  '--conf', 'spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem',
                  '--conf', 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension',
                  '--conf', 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' ]
    environment:
      PYSPARK_DRIVER_PYTHON: jupyter
      PYSPARK_DRIVER_PYTHON_OPTS: 'lab --ip=0.0.0.0'
    ports:
      - "8888:8888"
    networks:
      - lakehouse

volumes:
  postgres-data:
    driver: local
  minio-data:
    driver: local

networks:
  lakehouse: